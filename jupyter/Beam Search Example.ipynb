{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Implement a Beam Search Decoder for Natural Language Processing\n",
    "\n",
    "Natural language processing tasks, such as caption generation and machine translation, involve generating sequences of words.\n",
    "\n",
    "Models developed for these problems often operate by generating probability distributions across the vocabulary of output words and it is up to decoding algorithms to sample the probability distributions to generate the most likely sequences of words.\n",
    "\n",
    "In this tutorial, you will discover the greedy search and beam search decoding algorithms that can be used on text generation problems.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- The problem of decoding on text generation problems.\n",
    "- The greedy search decoder algorithm and how to implement it in Python.\n",
    "-  The beam search decoder algorithm and how to implement it in Python.\n",
    "\n",
    "### Decoder for Text Generation\n",
    "\n",
    "In natural language processing tasks such as caption generation, text summarization, and machine translation, the prediction required is a sequence of words.\n",
    "\n",
    "It is common for models developed for these types of problems to output a probability distribution over each word in the vocabulary for each word in the output sequence. It is then left to a decoder process to transform the probabilities into a final sequence of words.\n",
    "\n",
    "You are likely to encounter this when working with recurrent neural networks on natural language processing tasks where text is generated as an output. The final layer in the neural network model has one neuron for each word in the output vocabulary and a softmax activation function is used to output a likelihood of each word in the vocabulary being the next word in the sequence.\n",
    "\n",
    "Decoding the most likely output sequence involves searching through all the possible output sequences based on their likelihood. The size of the vocabulary is often tens or hundreds of thousands of words, or even millions of words. Therefore, the search problem is exponential in the length of the output sequence and is intractable (NP-complete) to search completely.\n",
    "\n",
    "In practice, heuristic search methods are used to return one or more approximate or “good enough” decoded output sequences for a given prediction.\n",
    "\n",
    "### Greedy Search Decoder\n",
    "\n",
    "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence.\n",
    "\n",
    "This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal.\n",
    "\n",
    "We can demonstrate the greedy search approach to decoding with a small contrived example in Python.\n",
    "\n",
    "We can start off with a prediction problem that involves a sequence of 10 words. Each word is predicted as a probability distribution over a vocabulary of 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define a sequence of 10 words over a vocab of 5 words\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy decoder\n",
    "def greedy_decoder(data):\n",
    "    # index for largest probability each row\n",
    "    return [argmax(s) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 4, 0, 4, 0, 4, 0, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# decode sequence\n",
    "result = greedy_decoder(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Decoder\n",
    "\n",
    "Another popular heuristic is the beam search that expands upon the greedy search and returns a list of most likely output sequences.\n",
    "\n",
    "Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
    "\n",
    "We do not need to start with random states; instead, we start with the k most likely words as the first step in the sequence.\n",
    "\n",
    "Common beam width values are 1 for a greedy search and values of 5 or 10 for common benchmark problems in machine translation. Larger beam widths result in better performance of a model as the multiple candidate sequences increase the likelihood of better matching a target sequence. This increased performance results in a decrease in decoding speed.\n",
    "\n",
    "The search process can halt for each candidate separately either by reaching a maximum length, by reaching an end-of-sequence token, or by reaching a threshold likelihood.\n",
    "\n",
    "Let’s make this concrete with an example.\n",
    "\n",
    "We can define a function to perform the beam search for a given sequence of probabilities and beam width parameter k. At each step, each candidate sequence is expanded with all possible next steps. Each candidate step is scored by multiplying the probabilities together. The k sequences with the most likely probabilities are selected and all other candidates are pruned. The process then repeats until the end of the sequence.\n",
    "\n",
    "Probabilities are small numbers and multiplying small numbers together creates very small numbers. To avoid underflowing the floating point numbers, the natural logarithm of the probabilities are multiplied together, which keeps the numbers larger and manageable. Further, it is also common to perform the search by minimizing the score, therefore, the negative log of the probabilities are multiplied. This final tweak means that we can sort all candidate sequences in ascending order by their score and select the first k as the most likely candidate sequences.\n",
    "\n",
    "The beam_search_decoder() function below implements the beam search decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 0.025600863289563108]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 0.03384250043584397]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 0.03384250043584397]\n"
     ]
    }
   ],
   "source": [
    "# decode sequence\n",
    "result = beam_search_decoder(data, 3)\n",
    "\n",
    "# print result\n",
    "for seq in result:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-utility-belt (env)",
   "language": "python",
   "name": "dl-utility-belt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
